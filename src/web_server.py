# [START FILE: abs-kosync-enhanced/web_server.py]
import glob
import html
import logging
import json
import os
import shutil
import subprocess
import sys
import threading
import time
from datetime import datetime
from pathlib import Path
from urllib.parse import urljoin

import requests
import schedule
from dependency_injector import providers
from flask import Flask, render_template, request, redirect, url_for, jsonify, session, send_from_directory

from src.utils.config_loader import ConfigLoader
from src.utils.logging_utils import memory_log_handler, LOG_PATH
from src.utils.logging_utils import sanitize_log_data
from src.utils.logging_utils import sanitize_log_data
from src.api.kosync_server import kosync_sync_bp, kosync_admin_bp, init_kosync_server
from src.api.hardcover_routes import hardcover_bp, init_hardcover_routes

def _reconfigure_logging():
    """Force update of root logger level based on env var."""
    try:
            new_level_str = os.environ.get('LOG_LEVEL', 'INFO').upper()
            new_level = getattr(logging, new_level_str, logging.INFO)

            root = logging.getLogger()
            root.setLevel(new_level)

            logger.info(f"üìù Logging level updated to {new_level_str}")
    except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to reconfigure logging: {e}")

# ---------------- APP SETUP ----------------
container = None
manager = None
database_service = None

def setup_dependencies(app, test_container=None):
    """
    Initialize dependencies for the web server.

    Args:
        test_container: Optional test container for dependency injection during testing.
                       If None, creates production container from environment.
    """
    global container, manager, database_service, DATA_DIR, EBOOK_DIR, COVERS_DIR

    # Initialize Database Service
    from src.db.migration_utils import initialize_database
    database_service = initialize_database(os.environ.get("DATA_DIR", "/data"))

    # Load settings from DB

    # This updates os.environ with values from the database
    if database_service:
        ConfigLoader.bootstrap_config(database_service)
        ConfigLoader.load_settings(database_service)
        logger.info("‚úÖ Settings loaded into environment variables")

        # Force reconfigure logging level based on new settings
        _reconfigure_logging()

    # RELOAD GLOBALS from updated os.environ

    global LINKER_BOOKS_DIR, DEST_BASE, STORYTELLER_INGEST, ABS_AUDIO_ROOT
    global STORYTELLER_LIBRARY_DIR, EBOOK_IMPORT_DIR
    global ABS_API_URL, ABS_API_TOKEN, ABS_LIBRARY_ID
    global ABS_COLLECTION_NAME, BOOKLORE_SHELF_NAME, MONITOR_INTERVAL, SHELFMARK_URL
    global SYNC_PERIOD_MINS, SYNC_DELTA_ABS_SECONDS, SYNC_DELTA_KOSYNC_PERCENT, FUZZY_MATCH_THRESHOLD

    LINKER_BOOKS_DIR = Path(os.environ.get("LINKER_BOOKS_DIR", "/linker_books"))
    DEST_BASE = Path(os.environ.get("PROCESSING_DIR", "/processing"))
    STORYTELLER_INGEST = Path(os.environ.get("STORYTELLER_INGEST_DIR", os.environ.get("LINKER_BOOKS_DIR", "/linker_books")))
    ABS_AUDIO_ROOT = Path(os.environ.get("AUDIOBOOKS_DIR", "/audiobooks"))
    STORYTELLER_LIBRARY_DIR = Path(os.environ.get("STORYTELLER_LIBRARY_DIR", "/storyteller_library"))
    EBOOK_IMPORT_DIR = Path(os.environ.get("EBOOK_IMPORT_DIR", "/books"))

    ABS_API_URL = os.environ.get("ABS_SERVER")
    ABS_API_TOKEN = os.environ.get("ABS_KEY")
    ABS_LIBRARY_ID = os.environ.get("ABS_LIBRARY_ID")

    def _get_float_env(key, default):
        try:
            return float(os.environ.get(key, str(default)))
        except (ValueError, TypeError):
            logger.warning(f"‚ö†Ô∏è Invalid '{key}' value, defaulting to {default}")
            return float(default)

    SYNC_PERIOD_MINS = _get_float_env("SYNC_PERIOD_MINS", 5)
    SYNC_DELTA_ABS_SECONDS = _get_float_env("SYNC_DELTA_ABS_SECONDS", 30)
    SYNC_DELTA_KOSYNC_PERCENT = _get_float_env("SYNC_DELTA_KOSYNC_PERCENT", 0.005)
    FUZZY_MATCH_THRESHOLD = _get_float_env("FUZZY_MATCH_THRESHOLD", 0.8)

    ABS_COLLECTION_NAME = os.environ.get("ABS_COLLECTION_NAME", "Synced with KOReader")
    BOOKLORE_SHELF_NAME = os.environ.get("BOOKLORE_SHELF_NAME", "Kobo")
    MONITOR_INTERVAL = int(os.environ.get("MONITOR_INTERVAL", "3600"))
    SHELFMARK_URL = os.environ.get("SHELFMARK_URL", "")

    logger.info(f"üîÑ Globals reloaded from settings (ABS_SERVER={ABS_API_URL})")

    if test_container is not None:
        # Use injected test container
        container = test_container
    else:
        # 3. Create production container AFTER loading settings
        # The container providers (Factories) will now read the updated os.environ values
        from src.utils.di_container import create_container
        container = create_container()

    # 4. Override the container's database_service with our already-initialized instance
    # This ensures consistency and prevents re-initialization
    # Only do this for production containers that support dependency injection
    if test_container is None:
        container.database_service.override(providers.Object(database_service))

    # Initialize manager and services
    manager = container.sync_manager()

    # Get data directories (now using updated env vars)
    DATA_DIR = container.data_dir()
    EBOOK_DIR = container.books_dir()

    # Initialize covers directory
    COVERS_DIR = DATA_DIR / "covers"
    if not COVERS_DIR.exists():
        COVERS_DIR.mkdir(parents=True, exist_ok=True)

    # Register KoSync Blueprint and initialize with dependencies
    init_kosync_server(database_service, container, manager, EBOOK_DIR)
    app.register_blueprint(kosync_sync_bp)
    app.register_blueprint(kosync_admin_bp)

    # Register Hardcover Blueprint and initialize with dependencies
    init_hardcover_routes(database_service, container)
    app.register_blueprint(hardcover_bp)

    logger.info(f"üöÄ Web server dependencies initialized (DATA_DIR={DATA_DIR})")







# Audiobook files location
ABS_AUDIO_ROOT = Path(os.environ.get("AUDIOBOOKS_DIR", "/audiobooks"))

# ABS API Configuration
ABS_API_URL = os.environ.get("ABS_SERVER")
ABS_API_TOKEN = os.environ.get("ABS_KEY")
ABS_LIBRARY_ID = os.environ.get("ABS_LIBRARY_ID")

# ABS Collection name for auto-adding matched books
ABS_COLLECTION_NAME = os.environ.get("ABS_COLLECTION_NAME", "Synced with KOReader")

# Booklore shelf name for auto-adding matched books
BOOKLORE_SHELF_NAME = os.environ.get("BOOKLORE_SHELF_NAME", "Kobo")


SHELFMARK_URL = os.environ.get("SHELFMARK_URL", "")

# Storyteller Forge
STORYTELLER_LIBRARY_DIR = Path(os.environ.get("STORYTELLER_LIBRARY_DIR", "/storyteller_library"))

# Track active forge operations for UI status
# Track active forge operations for UI status - MOVED TO FORGE SERVICE


# ---------------- HELPER FUNCTIONS ----------------
def get_audiobooks_conditionally():
    """Get audiobooks either from specific library or all libraries based on ABS_ONLY_SEARCH_IN_ABS_LIBRARY_ID setting."""
    abs_only_search_in_library = os.environ.get("ABS_ONLY_SEARCH_IN_ABS_LIBRARY_ID", "false").lower() == "true"
    abs_library_id = os.environ.get("ABS_LIBRARY_ID")

    if abs_only_search_in_library and abs_library_id:
        # Fetch audiobooks only from the specified library
        return container.abs_client().get_audiobooks_for_lib(abs_library_id)
    else:
        # Fetch all audiobooks from all libraries
        return container.abs_client().get_all_audiobooks()

# ---------------- CONTEXT PROCESSORS ----------------
def inject_global_vars():
    def get_val(key, default_val=None):
        if key in os.environ: return os.environ[key]
        DEFAULTS = {
            'TZ': 'America/New_York',
            'LOG_LEVEL': 'INFO',
            'DATA_DIR': '/data',
            'BOOKS_DIR': '/books',
            'ABS_COLLECTION_NAME': 'Synced with KOReader',
            'BOOKLORE_SHELF_NAME': 'Kobo',
            'SYNC_PERIOD_MINS': '5',
            'SYNC_DELTA_ABS_SECONDS': '60',
            'SYNC_DELTA_KOSYNC_PERCENT': '0.5',
            'SYNC_DELTA_BETWEEN_CLIENTS_PERCENT': '0.5',
            'SYNC_DELTA_KOSYNC_WORDS': '400',
            'FUZZY_MATCH_THRESHOLD': '80',
            'WHISPER_MODEL': 'tiny',
            'JOB_MAX_RETRIES': '5',
            'JOB_RETRY_DELAY_MINS': '15',
            'MONITOR_INTERVAL': '3600',
            'LINKER_BOOKS_DIR': '/linker_books',
            'PROCESSING_DIR': '/processing',
            'STORYTELLER_INGEST_DIR': '/linker_books',
            'AUDIOBOOKS_DIR': '/audiobooks',
            'ABS_PROGRESS_OFFSET_SECONDS': '0',
            'EBOOK_CACHE_SIZE': '3',
            'KOSYNC_HASH_METHOD': 'content',
            'TELEGRAM_LOG_LEVEL': 'ERROR',
            'SHELFMARK_URL': '',
            'KOSYNC_ENABLED': 'false',
            'STORYTELLER_ENABLED': 'false',
            'BOOKLORE_ENABLED': 'false',
            'HARDCOVER_ENABLED': 'false',
            'TELEGRAM_ENABLED': 'false',
            'SUGGESTIONS_ENABLED': 'false',
            'REPROCESS_ON_CLEAR_IF_NO_ALIGNMENT': 'true'
        }
        if key in DEFAULTS: return DEFAULTS[key]
        return default_val if default_val is not None else ''

    def get_bool(key):
        val = os.environ.get(key, 'false')
        return val.lower() in ('true', '1', 'yes', 'on')

    return dict(
        shelfmark_url=os.environ.get("SHELFMARK_URL", ""),
        abs_server=os.environ.get("ABS_SERVER", ""),
        booklore_server=os.environ.get("BOOKLORE_SERVER", ""),
        get_val=get_val,
        get_bool=get_bool
    )

# ---------------- BOOK LINKER HELPERS ----------------







def sync_daemon():
    """Background sync daemon running in a separate thread."""
    try:
        # Setup schedule for sync operations
        # Use the global SYNC_PERIOD_MINS which is validated
        schedule.every(int(SYNC_PERIOD_MINS)).minutes.do(manager.sync_cycle)
        schedule.every(1).minutes.do(manager.check_pending_jobs)

        logger.info(f"üîÑ Sync daemon started (period: {SYNC_PERIOD_MINS} minutes)")

        # Run initial sync cycle
        try:
            manager.sync_cycle()
        except Exception as e:
            logger.error(f"‚ùå Initial sync cycle failed: {e}")

        # Main daemon loop
        while True:
            try:
                # logger.debug("Running pending schedule jobs...")
                schedule.run_pending()
                time.sleep(30)  # Check every 30 seconds
            except Exception as e:
                logger.error(f"‚ùå Sync daemon error: {e}")
                time.sleep(60)  # Wait longer on error

    except Exception as e:
        logger.error(f"‚ùå Sync daemon crashed: {e}")


# ---------------- ORIGINAL ABS-KOSYNC HELPERS ----------------

def find_ebook_file(filename):
    base = EBOOK_DIR
    escaped_filename = glob.escape(filename)
    matches = list(base.rglob(escaped_filename))
    return matches[0] if matches else None


def get_kosync_id_for_ebook(ebook_filename, booklore_id=None, original_filename=None):
    """Get KOSync document ID for an ebook.
    Tries Booklore API first (if configured and booklore_id provided),
    falls back to filesystem if needed.
    """
    # Try Booklore API first
    if booklore_id and container.booklore_client().is_configured():
        try:
            content = container.booklore_client().download_book(booklore_id)
            if content:
                kosync_id = container.ebook_parser().get_kosync_id_from_bytes(ebook_filename, content)
                if kosync_id:
                    logger.debug(f"üîç Computed KOSync ID from Booklore download: '{kosync_id}'")
                    return kosync_id
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to get KOSync ID from Booklore, falling back to filesystem: {e}")

    # Fall back to filesystem
    ebook_path = find_ebook_file(ebook_filename)
    if not ebook_path and original_filename:
        # [Tri-Link] Fallback to original filename if Storyteller file not found/relevant
        logger.debug(f"Primary file '{ebook_filename}' not found, checking original '{original_filename}'")
        ebook_path = find_ebook_file(original_filename)

    if ebook_path:
        return container.ebook_parser().get_kosync_id(ebook_path)

    # [NEW] Check Epub Cache explicitly (if acquired by LibraryService but not meant for /books)
    epub_cache = container.epub_cache_dir()
    cached_path = epub_cache / ebook_filename
    if cached_path.exists():
         return container.ebook_parser().get_kosync_id(cached_path)

    # [NEW] On-Demand Fetching
    # 1. ABS On-Demand
    if "_abs." in ebook_filename:
        try:
             # Extract ID: 1941a138-1c8d-49eb-954f-f6bb26f87ebc_abs.epub -> 1941a138-1c8d-49eb-954f-f6bb26f87ebc
             abs_id = ebook_filename.split("_abs.")[0]
             abs_client = container.abs_client()
             if abs_client and abs_client.is_configured():
                 logger.info(f"üì• Attempting on-demand ABS download for '{abs_id}'")
                 ebook_files = abs_client.get_ebook_files(abs_id)
                 if ebook_files:
                     target = ebook_files[0]
                     if not epub_cache.exists(): epub_cache.mkdir(parents=True, exist_ok=True)
                     
                     if abs_client.download_file(target['stream_url'], cached_path):
                         logger.info(f"   ‚úÖ Downloaded ABS ebook to '{cached_path}'")
                         return container.ebook_parser().get_kosync_id(cached_path)
                 else:
                     logger.warning(f"   ‚ö†Ô∏è No ebook files found in ABS for item '{abs_id}'")
        except Exception as e:
            logger.error(f"   ‚ùå Failed ABS on-demand download: {e}")

    # 2. CWA On-Demand
    if "_cwa." in ebook_filename or ebook_filename.startswith("cwa_"):
        try:
             # Extract ID: cwa_12345.epub -> 12345
             # Format is cwa_{id}.{ext}
             if ebook_filename.startswith("cwa_"):
                 # Robust: strip cwa_ prefix and the extension
                 cwa_id = ebook_filename[4:].rsplit(".", 1)[0]
             else:
                 # Pattern like somefile_cwa.epub or itemid_cwa.epub
                 cwa_id = ebook_filename.split("_cwa.")[0]
                 
                 # If it was still prefixed with something else, handle it? 
                 # Usually it's {uuid}_cwa.epub or cwa_{id}.epub
                 if "_" in cwa_id and not ebook_filename.startswith("cwa_"):
                     # If format is uuid_cwa.epub, cwa_id is uuid (correct)
                     pass 
                 
             if cwa_id:
                 cwa_client = container.cwa_client()
                 if cwa_client and cwa_client.is_configured():
                     logger.info(f"üì• Attempting on-demand CWA download for ID '{cwa_id}'")
                     
                     target = None
                     
                     # Priority 1: Search for the ID (search results include download_url and won't crash the server)
                     results = cwa_client.search_ebooks(cwa_id)
                     
                     # Find exact ID match if possible
                     for res in results:
                         if str(res.get('id')) == cwa_id:
                             target = res
                             break
                     
                     # If no exact ID match, maybe it was the only result
                     if not target and len(results) == 1:
                         target = results[0]

                     # Priority 2: Use direct download URL from search if available
                     if target and target.get('download_url'):
                         logger.info(f"üöÄ Using direct download link from search for '{target.get('title', 'Unknown')}'")
                     else:
                         # Priority 3: Fallback to get_book_by_id only if search didn't provide a URL
                         # This may crash server on metadata page, but includes a blind URL fallback
                         logger.debug(f"üîç Search did not return a usable result, trying direct ID lookup")
                         target = cwa_client.get_book_by_id(cwa_id)

                     if target and target.get('download_url'):
                         if not epub_cache.exists(): epub_cache.mkdir(parents=True, exist_ok=True)
                         if cwa_client.download_ebook(target['download_url'], cached_path):
                             logger.info(f"   ‚úÖ Downloaded CWA ebook to '{cached_path}'")
                             return container.ebook_parser().get_kosync_id(cached_path)
                     else:
                         logger.warning(f"   ‚ö†Ô∏è Could not find CWA book for ID '{cwa_id}'")
        except Exception as e:
            logger.error(f"   ‚ùå Failed CWA on-demand download: {e}")

    # Neither source available - log helpful warning
    if not container.booklore_client().is_configured() and not EBOOK_DIR.exists():
        logger.warning(
            f"‚ö†Ô∏è Cannot compute KOSync ID for '{ebook_filename}': "
            "Neither Booklore integration nor /books volume is configured. "
            "Enable Booklore (BOOKLORE_SERVER, BOOKLORE_USER, BOOKLORE_PASSWORD) "
            "or mount the ebooks directory to /books"
        )
    elif not booklore_id and not ebook_path:
        logger.warning(f"‚ö†Ô∏è Cannot compute KOSync ID for '{ebook_filename}': File not found in Booklore, filesystem, or remote sources")

    return None


class EbookResult:
    """Wrapper to provide consistent interface for ebooks from Booklore, CWA, ABS, or filesystem."""

    def __init__(self, name, title=None, subtitle=None, authors=None, booklore_id=None, path=None, source=None, source_id=None):
        self.name = name
        self.title = title or Path(name).stem
        self.subtitle = subtitle or ''
        self.authors = authors or ''
        self.booklore_id = booklore_id
        self.path = path # Public path
        self.source = source  # 'booklore', 'cwa', 'abs', 'filesystem'
        self.source_id = source_id or booklore_id # Generic ID for any source
        # Has metadata if we have a real title (not just filename) or booklore_id
        self.has_metadata = booklore_id is not None or (title is not None and title != name)

    @property
    def display_name(self):
        """Format: 'Author - Title: Subtitle' for sources with metadata, filename for filesystem."""
        if self.has_metadata and self.title:
            full_title = self.title
            if self.subtitle:
                full_title = f"{self.title}: {self.subtitle}"
            if self.authors:
                return f"{self.authors} - {full_title}"
            return full_title
        return self.name

    @property
    def stem(self):
        return Path(self.name).stem

    def __str__(self):
        return self.name


def get_searchable_ebooks(search_term):
    """Get ebooks from Booklore API, filesystem, ABS, and CWA.
    Returns list of EbookResult objects for consistent interface."""

    results = []
    found_filenames = set()
    found_stems = set()  # To dedupe by title stem

    # 1. Booklore
    if container.booklore_client().is_configured():
        try:
            books = container.booklore_client().search_books(search_term)
            if books:
                for b in books:
                    fname = b.get('fileName', '')
                    if fname.lower().endswith('.epub'):
                        found_filenames.add(fname.lower())
                        found_stems.add(Path(fname).stem.lower())
                        results.append(EbookResult(
                            name=fname,
                            title=b.get('title'),
                            subtitle=b.get('subtitle'),
                            authors=b.get('authors'),
                            booklore_id=b.get('id'),
                            source='Booklore'
                        ))
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Booklore search failed: {e}")

    # 2. ABS ebook libraries
    if search_term:
        try:
            abs_client = container.abs_client()
            if abs_client:
                abs_ebooks = abs_client.search_ebooks(search_term)
                if abs_ebooks:
                    for ab in abs_ebooks:
                        ebook_files = abs_client.get_ebook_files(ab['id'])
                        if ebook_files:
                            ef = ebook_files[0]
                            fname = f"{ab['id']}_abs.{ef['ext']}"
                            if fname.lower() not in found_filenames:
                                results.append(EbookResult(
                                    name=fname,
                                    title=ab.get('title'),
                                    authors=ab.get('author'),
                                    source='ABS',
                                    source_id=ab.get('id')
                                ))
                                found_filenames.add(fname.lower())
                                if ab.get('title'):
                                    found_stems.add(ab['title'].lower().strip())
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ABS ebook search failed: {e}")

    # 3. CWA (Calibre-Web Automated)
    if search_term:
        try:
            library_service = container.library_service()
            if library_service and library_service.cwa_client and library_service.cwa_client.is_configured():
                cwa_results = library_service.cwa_client.search_ebooks(search_term)
                if cwa_results:
                    for cr in cwa_results:
                        fname = f"cwa_{cr.get('id', 'unknown')}.{cr.get('ext', 'epub')}"
                        if fname.lower() not in found_filenames:
                            results.append(EbookResult(
                                name=fname,
                                title=cr.get('title'),
                                authors=cr.get('author'),
                                source='CWA',
                                source_id=cr.get('id')
                            ))
                            found_filenames.add(fname.lower())
                            if cr.get('title'):
                                found_stems.add(cr['title'].lower().strip())
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è CWA search failed: {e}")

    # 4. Search filesystem (Local) - LOW PRIORITY
    if EBOOK_DIR.exists():
        try:
            all_epubs = list(EBOOK_DIR.glob("**/*.epub"))
            for eb in all_epubs:
                fname_lower = eb.name.lower()
                stem_lower = eb.stem.lower()

                # Dedupe: if already found in rich source, skip
                if fname_lower in found_filenames or stem_lower in found_stems:
                    continue

                if not search_term or search_term.lower() in fname_lower:
                    results.append(EbookResult(name=eb.name, path=eb, source='Local File'))
                    found_filenames.add(fname_lower)
                    found_stems.add(stem_lower)

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Filesystem search failed: {e}")

    # Check if we have no sources at all
    if not results and not EBOOK_DIR.exists() and not container.booklore_client().is_configured():
        logger.warning(
            "‚ö†Ô∏è No ebooks available: Neither Booklore integration nor /books volume is configured. "
            "Enable Booklore (BOOKLORE_SERVER, BOOKLORE_USER, BOOKLORE_PASSWORD) "
            "or mount the ebooks directory to /books"
        )

    return results



def restart_server():
    """
    Triggers a graceful restart by sending SIGTERM to the current process.
    The start.sh supervisor loop will catch the exit and restart the application.
    """
    logger.info("‚ôªÔ∏è  Stopping application (Supervisor will restart it)...")
    time.sleep(1.0)  # Give Flask time to send the redirect response

    # Exit with 0 so start.sh loop restarts the process
    logger.info("üëã Exiting process to trigger restart...")
    sys.exit(0)

def settings():
    # Application Defaults
    # Note: These are also defined in inject_global_vars for context processor usage
    # We should probably centralize them, but for now this works.

    if request.method == 'POST':
        bool_keys = [
            'KOSYNC_USE_PERCENTAGE_FROM_SERVER',
            'SYNC_ABS_EBOOK',
            'XPATH_FALLBACK_TO_PREVIOUS_SEGMENT',
            'KOSYNC_ENABLED',
            'STORYTELLER_ENABLED',
            'BOOKLORE_ENABLED',
            'CWA_ENABLED',
            'HARDCOVER_ENABLED',
            'TELEGRAM_ENABLED',
            'SUGGESTIONS_ENABLED',
            'ABS_ONLY_SEARCH_IN_ABS_LIBRARY_ID',
            'REPROCESS_ON_CLEAR_IF_NO_ALIGNMENT'
        ]

        # Current settings in DB
        current_settings = database_service.get_all_settings()

        # 1. Handle Boolean Toggles (Checkbox logic)
        # Checkboxes are NOT sent if unchecked, so we must check every known bool key
        for key in bool_keys:
            is_checked = (key in request.form)
            # Save "true" or "false"
            val_str = str(is_checked).lower()
            database_service.set_setting(key, val_str)
            os.environ[key] = val_str # Immediate update for current process

        # 2. Handle Text Inputs
        # Iterate over form to find other keys
        for key, value in request.form.items():
            if key in bool_keys: continue

            clean_value = value.strip()

            if clean_value:
                database_service.set_setting(key, clean_value)
                os.environ[key] = clean_value # Immediate update for current process
            elif key in current_settings:
                database_service.set_setting(key, "")
                os.environ[key] = "" # Immediate update for current process

        try:
            # Trigger Auto-Restart in a separate thread so this request finishes
            threading.Thread(target=restart_server).start()

            session['message'] = "Settings saved. Application is restarting..."
            session['is_error'] = False
        except Exception as e:
            session['message'] = f"Error saving settings: {e}"
            session['is_error'] = True
            logger.error(f"‚ùå Error saving settings: {e}")

        return redirect(url_for('settings'))

    # GET Request
    message = session.pop('message', None)
    is_error = session.pop('is_error', False)

    return render_template('settings.html',
                         message=message,
                         is_error=is_error)

def get_abs_author(ab):

    """Extract author from ABS audiobook metadata."""
    media = ab.get('media', {})
    metadata = media.get('metadata', {})
    return metadata.get('authorName') or (metadata.get('authors') or [{}])[0].get("name", "")


def audiobook_matches_search(ab, search_term):
    """Check if audiobook matches search term (searches title AND author)."""
    import re

    # Normalize: remove punctuation
    def normalize(s):
        return re.sub(r'[^\w\s]', '', s.lower())

    title = normalize(manager.get_abs_title(ab))
    author = normalize(get_abs_author(ab))
    search_norm = normalize(search_term)

    # 1. Standard Search: Search term is in Title or Author (e.g. "Harry" in "Harry Potter")
    if search_norm in title or search_norm in author:
        return True

    # 2. Reverse Search: Title/Author is in Search term (e.g. "Dune" in "Dune Messiah")
    # FIX: Enforce minimum length to prevent short/empty matches (e.g. "The", "It", "")
    MIN_LEN = 4
    
    if len(title) >= MIN_LEN and title in search_norm: return True
    if len(author) >= MIN_LEN and author in search_norm: return True

    return False

# ---------------- ROUTES ----------------
def index():
    """Dashboard - loads books and progress from database service"""

    # Load books from database service
    books = database_service.get_all_books()

    # Fetch all states at once to avoid N+1 queries with NullPool
    all_states = database_service.get_all_states()
    states_by_book = {}
    for state in all_states:
        if state.abs_id not in states_by_book:
            states_by_book[state.abs_id] = []
        states_by_book[state.abs_id].append(state)

    # Fetch pending suggestions
    suggestions_raw = database_service.get_all_pending_suggestions()

    # Filter suggestions: Hide those with 0 matches
    suggestions = []

    for s in suggestions_raw:
        if len(s.matches) == 0:
            continue
        suggestions.append(s)

    # [OPTIMIZATION] Fetch all hardcover details at once
    all_hardcover = database_service.get_all_hardcover_details()
    hardcover_by_book = {h.abs_id: h for h in all_hardcover}

    integrations = {}

    # Dynamically check all configured sync clients
    sync_clients = container.sync_clients()
    for client_name, client in sync_clients.items():
        if client.is_configured():
            integrations[client_name.lower()] = True
        else:
            integrations[client_name.lower()] = False

    # Convert books to mappings format for template compatibility
    mappings = []
    total_duration = 0
    total_listened = 0

    for book in books:
        # Get states for this book from pre-fetched dict
        states = states_by_book.get(book.abs_id, [])

        # Convert states to a dict by client name for easy access
        state_by_client = {state.client_name: state for state in states}

        # Create mapping dict for template compatibility
        mapping = {
            'abs_id': book.abs_id,
            'abs_title': book.abs_title,
            'ebook_filename': book.ebook_filename,
            'kosync_doc_id': book.kosync_doc_id,
            'transcript_file': book.transcript_file,
            'status': book.status,
            'sync_mode': getattr(book, 'sync_mode', 'audiobook'),
            'unified_progress': 0,
            'duration': book.duration or 0,
            'storyteller_uuid': book.storyteller_uuid,
            'states': {}
        }

        if book.status == 'processing':
            job = database_service.get_latest_job(book.abs_id)
            if job:
                mapping['job_progress'] = round((job.progress or 0.0) * 100, 1)
            else:
                mapping['job_progress'] = 0.0

        # Populate progress from states
        latest_update_time = 0
        max_progress = 0

        # Process each client state and store both timestamp and percentage
        for client_name, state in state_by_client.items():
            if state.last_updated and state.last_updated > latest_update_time:
                latest_update_time = state.last_updated

            # Store both timestamp and percentage for each client
            mapping['states'][client_name] = {
                'timestamp': state.timestamp or 0,
                'percentage': round(state.percentage * 100, 1) if state.percentage else 0,
                'last_updated': state.last_updated
            }

            # Calculate max progress for unified_progress (using percentage)
            if state.percentage:
                progress_pct = round(state.percentage * 100, 1)
                max_progress = max(max_progress, progress_pct)

        # Add hardcover mapping details
        hardcover_details = hardcover_by_book.get(book.abs_id)
        if hardcover_details:
            mapping.update({
                'hardcover_book_id': hardcover_details.hardcover_book_id,
                'hardcover_slug': hardcover_details.hardcover_slug,
                'hardcover_edition_id': hardcover_details.hardcover_edition_id,
                'hardcover_pages': hardcover_details.hardcover_pages,
                'isbn': hardcover_details.isbn,
                'asin': hardcover_details.asin,
                'matched_by': hardcover_details.matched_by,
                'hardcover_linked': True,
                'hardcover_title': book.abs_title  # Use ABS title as fallback for Hardcover title
            })
        else:
            mapping.update({
                'hardcover_book_id': None,
                'hardcover_slug': None,
                'hardcover_edition_id': None,
                'hardcover_pages': None,
                'isbn': None,
                'asin': None,
                'matched_by': None,
                'hardcover_linked': False,
                'hardcover_title': None
            })
            
        # [NEW] Check for legacy Storyteller link
        # Book has 'storyteller' state but no 'storyteller_uuid'
        has_storyteller_state = 'storyteller' in state_by_client
        is_legacy_link = has_storyteller_state and not book.storyteller_uuid
        mapping['storyteller_legacy_link'] = is_legacy_link

        # Platform deep links for dashboard
        mapping['abs_url'] = f"{manager.abs_client.base_url}/item/{book.abs_id}"

        # Booklore deep link (if configured and book found)
        if manager.booklore_client.is_configured():
            bl_book = manager.booklore_client.find_book_by_filename(book.ebook_filename, allow_refresh=False)
            # [FIX] Fallback to original filename if storyteller artifact doesn't match
            if not bl_book and book.original_ebook_filename:
                bl_book = manager.booklore_client.find_book_by_filename(book.original_ebook_filename, allow_refresh=False)
        else:
            bl_book = None

        if bl_book:
            mapping['booklore_id'] = bl_book.get('id')
            mapping['booklore_url'] = f"{manager.booklore_client.base_url}/book/{bl_book.get('id')}?tab=view"
        else:
            mapping['booklore_id'] = None
            mapping['booklore_url'] = None

        # Hardcover deep link (if linked)
        if mapping.get('hardcover_slug'):
            mapping['hardcover_url'] = f"https://hardcover.app/books/{mapping['hardcover_slug']}"
        elif mapping.get('hardcover_book_id'):
            mapping['hardcover_url'] = f"https://hardcover.app/books/{mapping['hardcover_book_id']}"
        else:
            mapping['hardcover_url'] = None

        # Set unified progress to the maximum progress across all clients
        mapping['unified_progress'] = min(max_progress, 100.0)

        # Calculate last sync time
        if latest_update_time > 0:
            diff = time.time() - latest_update_time
            if diff < 60:
                mapping['last_sync'] = f"{int(diff)}s ago"
            elif diff < 3600:
                mapping['last_sync'] = f"{int(diff // 60)}m ago"
            else:
                mapping['last_sync'] = f"{int(diff // 3600)}h ago"
        else:
            mapping['last_sync'] = "Never"

        # Set cover URL
        if book.abs_id:
            mapping['cover_url'] = f"{manager.abs_client.base_url}/api/items/{book.abs_id}/cover?token={manager.abs_client.token}"

        # Add to totals for overall progress calculation
        duration = mapping.get('duration', 0)
        progress_pct = mapping.get('unified_progress', 0)

        if duration > 0:
            total_duration += duration
            total_listened += (progress_pct / 100.0) * duration

        mappings.append(mapping)

    # Calculate overall progress based on total duration and listening time
    if total_duration > 0:
        overall_progress = round((total_listened / total_duration) * 100, 1)
    elif mappings:
        # Fallback: average progress if no duration data available
        overall_progress = round(sum(m['unified_progress'] for m in mappings) / len(mappings), 1)
    else:
        overall_progress = 0

    return render_template('index.html', mappings=mappings, integrations=integrations, progress=overall_progress, suggestions=suggestions)


def shelfmark():
    """Shelfmark view - renders an iframe with SHELFMARK_URL"""
    url = os.environ.get("SHELFMARK_URL")
    if not url:
        return redirect(url_for('index'))
    return render_template('shelfmark.html', shelfmark_url=url)


def forge():
    """Storyteller Forge - 2-column UI for combining ABS audio with ebook text."""
    return render_template('forge.html')


def forge_search_audio():
    """API: Search ABS audiobooks for Forge (returns JSON)."""
    query = request.args.get('q', '').strip()
    if not query:
        return jsonify([])

    try:
        all_audiobooks = get_audiobooks_conditionally()
        query_lower = query.lower()
        results = []

        for ab in all_audiobooks:
            if audiobook_matches_search(ab, query_lower):
                item_details = container.abs_client().get_item_details(ab.get('id'))
                if not item_details:
                    continue

                media = item_details.get('media', {})
                metadata = media.get('metadata', {})
                audio_files = media.get('audioFiles', [])
                title = metadata.get('title', ab.get('name', 'Unknown'))

                if not audio_files:
                    continue

                size_mb = sum(f.get('metadata', {}).get('size', 0) for f in audio_files) / (1024 * 1024)

                # Build cover URL
                cover_url = ""
                abs_server = os.environ.get("ABS_SERVER", "")
                if abs_server:
                    cover_url = f"/api/cover-proxy/{ab.get('id')}"

                results.append({
                    "id": ab.get("id"),
                    "title": title,
                    "author": metadata.get('authorName') or get_abs_author(ab),
                    "file_size_mb": round(size_mb, 2),
                    "num_files": len(audio_files),
                    "cover_url": cover_url,
                })

        return jsonify(results)
    except Exception as e:
        logger.error(f"‚ùå Forge audio search failed: {e}", exc_info=True)
        return jsonify([])


def forge_search_text():
    """API: Unified text source search for Forge - ABS ebooks, Booklore, CWA, local files."""
    query = request.args.get('q', '').strip()
    if not query:
        return jsonify([])

    results = []
    found_ids = set()  # Dedupe
    query_lower = query.lower()

    # 1. Booklore
    if container.booklore_client().is_configured():
        try:
            books = container.booklore_client().search_books(query)
            if books:
                for b in books:
                    fname = b.get('fileName', '')
                    if fname.lower().endswith('.epub'):
                        key = f"booklore_{b.get('id', fname)}"
                        if key not in found_ids:
                            found_ids.add(key)
                            results.append({
                                "id": key,
                                "title": b.get('title', fname),
                                "author": b.get('authors', ''),
                                "source": "Booklore",
                                "filename": fname,
                                "booklore_id": b.get('id'),
                            })
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Forge: Booklore search failed: {e}")

    # 2. ABS Ebooks
    try:
        abs_client = container.abs_client()
        if abs_client:
            abs_ebooks = abs_client.search_ebooks(query)
            if abs_ebooks:
                for ab in abs_ebooks:
                    ebook_files = abs_client.get_ebook_files(ab['id'])
                    if ebook_files:
                        ef = ebook_files[0]
                        key = f"abs_{ab['id']}"
                        if key not in found_ids:
                            found_ids.add(key)
                            results.append({
                                "id": key,
                                "title": ab.get('title', 'Unknown'),
                                "author": ab.get('author', ''),
                                "source": "ABS",
                                "abs_id": ab['id'],
                                "ext": ef.get('ext', 'epub'),
                            })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Forge: ABS ebook search failed: {e}")

    # 3. CWA
    try:
        library_service = container.library_service()
        if library_service and library_service.cwa_client and library_service.cwa_client.is_configured():
            cwa_results = library_service.cwa_client.search_ebooks(query)
            if cwa_results:
                for cr in cwa_results:
                    key = f"cwa_{cr.get('id', 'unknown')}"
                    if key not in found_ids:
                        found_ids.add(key)
                        results.append({
                            "id": key,
                            "title": cr.get('title', 'Unknown'),
                            "author": cr.get('author', ''),
                            "source": "CWA",
                            "cwa_id": cr.get('id'),
                            "ext": cr.get('ext', 'epub'),
                            "download_url": cr.get('download_url', ''),
                        })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Forge: CWA search failed: {e}")

    # 4. Local files from BOOKS_DIR
    try:
        local_books_dir = Path(os.environ.get("BOOKS_DIR", "/books"))
        if local_books_dir.exists():
            for epub in local_books_dir.rglob("*.epub"):
                if "(readaloud)" in epub.name.lower():
                    continue
                if query_lower in epub.name.lower():
                    key = f"local_{epub.name}"
                    if key not in found_ids:
                        found_ids.add(key)
                        results.append({
                            "id": key,
                            "title": epub.stem,
                            "author": "",
                            "source": "Local File",
                            "path": str(epub),
                            "file_size_mb": round(epub.stat().st_size / (1024 * 1024), 2),
                        })
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Forge: Local file search failed: {e}")

    return jsonify(results)





def forge_process():
    """API: Start the forge process (copy files + cleanup in background)."""
    data = request.get_json()
    if not data:
        return jsonify({"error": "Missing JSON payload"}), 400

    abs_id = data.get('abs_id')
    text_item = data.get('text_item')

    if not abs_id or not text_item:
        return jsonify({"error": "Missing abs_id or text_item"}), 400

    # Get title/author from ABS for folder naming
    title = "Unknown"
    author = "Unknown"
    try:
        item_details = container.abs_client().get_item_details(abs_id)
        if item_details:
            metadata = item_details.get('media', {}).get('metadata', {})
            title = metadata.get('title', 'Unknown')
            author = metadata.get('authorName', '') or get_abs_author(item_details) or 'Unknown'
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Forge: Could not get ABS metadata for '{abs_id}': {e}")

    # Start manual forge in service
    try:
        container.forge_service().start_manual_forge(abs_id, text_item, title, author)
        msg = f"Forge started for '{title}'. Processing and cleanup running in background."
    except Exception as e:
        logger.error(f"‚ùå Failed to start forge: {e}")
        return jsonify({"error": f"Failed to start forge: {e}"}), 500

    return jsonify({
        "message": msg,
        "title": title,
        "author": author,
    }), 202


def match():
    if request.method == 'POST':
        abs_id = request.form.get('audiobook_id')
        selected_filename = request.form.get('ebook_filename')
        ebook_filename = selected_filename
        original_ebook_filename = None
        audiobooks = container.abs_client().get_all_audiobooks()
        selected_ab = next((ab for ab in audiobooks if ab['id'] == abs_id), None)
        if not selected_ab: return "Audiobook not found", 404

        # Get booklore_id if available for API-based hash computation
        booklore_id = None
        
        # [NEW ACTION] Forge & Match
        if request.form.get('action') == 'forge_match':
            original_filename = request.form.get('ebook_filename')
            if not original_filename:
                return "Original ebook filename required for forge match", 400
                
            # 1. Prepare text item (reconstruct from form/source logic or assume passed)
            # Actually, standard match doesn't have 'text_item' logic fully exposed in form? 
            # The forge UI sends text_item JSON. 
            # But here we are arguably coming from the match page. 
            # If we add "Forge" button, we need to know the SOURCE of the text.
            # Assuming the form includes necessary details or we can infer.
            # [SIMPLIFICATION] For now, assume 'ebook_filename' is a valid Local text file from search?
            # Or is this action coming from the Forge modal? No, "Forge & Match".
            # If it's from the match page, the user selected an ebook result.
            # We need to reconstruct the `text_item` dict expected by ForgeService.
            
            # Extract source details from form (hidden inputs?)
            # We'll need to update match.html to send these.
            source_type = request.form.get('source_type')
            source_path = request.form.get('source_path') 
            source_id = request.form.get('source_id') # booklore id, cwa id, etc
            
            text_item = {
                "source": source_type,
                "path": source_path,
                "booklore_id": source_id,
                "cwa_id": source_id,
                "abs_id": source_id, # ambiguous but handled by specific keys
                "filename": original_filename
            }
            
            # Map specific keys based on source
            if source_type == 'ABS': text_item['abs_id'] = source_id
            if source_type == 'Booklore': text_item['booklore_id'] = source_id
            if source_type == 'CWA': text_item['cwa_id'] = source_id
            if source_type == 'Local File': text_item['path'] = source_path
            
            # 2. Calculate initial Kosync ID (Original) - strictly for DB record
            # We use the ORIGINAL file for the ID initially (or forever if tri-linked).
            kosync_doc_id = get_kosync_id_for_ebook(original_filename, None)
            
            if not kosync_doc_id:
                # If we can't get ID from original (e.g. remote only?), we might rely on the forged one later.
                # But we need a DB record now.
                # Generate a temporary or hash-based ID? Or fail?
                # Failing is safer.
                logger.warning(f"‚ö†Ô∏è Could not compute ID for original '{original_filename}'")
                # return "Could not compute KOSync ID for original file", 400
                # Actually, `start_auto_forge_match` can update it? 
                # Let's proceed with a placeholder or fail.
                # Use a specific error.
                pass 

            from src.db.models import Book
            # Create dummy book record with status='forging'
            book = Book(
                abs_id=abs_id,
                abs_title=manager.get_abs_title(selected_ab),
                ebook_filename=original_filename,
                original_ebook_filename=original_filename,
                kosync_doc_id=kosync_doc_id or f"forging_{abs_id}", # temporary
                status="forging",
                duration=manager.get_duration(selected_ab)
            )
            database_service.save_book(book)
            
            # Start Auto-Forge
            author = get_abs_author(selected_ab)
            title = manager.get_abs_title(selected_ab)
            
            # Async launch
            container.forge_service().start_auto_forge_match(
                abs_id=abs_id,
                text_item=text_item,
                title=title,
                author=author,
                original_filename=original_filename,
                original_hash=kosync_doc_id
            )
            
            # Dismiss pending suggestion if it exists (for both ABS ID and potential KOSync ID)
            # This cleans up the suggestions list immediately upon starting the forge process
            database_service.dismiss_suggestion(abs_id)
            if kosync_doc_id:
                database_service.dismiss_suggestion(kosync_doc_id)

            return redirect(url_for('index'))
            
        # [NEW] Storyteller Tri-Link Logic
        storyteller_uuid = request.form.get('storyteller_uuid')
        
        if storyteller_uuid:
            # If Storyteller UUID is selected, we prioritize it
            try:
                epub_cache = container.epub_cache_dir()
                if not epub_cache.exists(): epub_cache.mkdir(parents=True, exist_ok=True)
                
                target_filename = f"storyteller_{storyteller_uuid}.epub"
                target_path = epub_cache / target_filename
                
                logger.info(f"üîç Using Storyteller Artifact: '{storyteller_uuid}'")
                
                if container.storyteller_client().download_book(storyteller_uuid, target_path):
                    ebook_filename = target_filename # Override filename
                    original_ebook_filename = selected_filename # Preserve original
                    
                    # [FIX] Conditionally compute KOSync ID
                    if original_ebook_filename:
                        # Tri-Link: Compute hash from the normal EPUB so it matches the user's device
                        logger.info(f"‚ö° Tri-Link: Computing hash from original EPUB '{original_ebook_filename}'")
                        booklore_id = None
                        if container.booklore_client().is_configured():
                            bl_book = container.booklore_client().find_book_by_filename(original_ebook_filename)
                            if bl_book:
                                booklore_id = bl_book.get('id')
                        kosync_doc_id = get_kosync_id_for_ebook(original_ebook_filename, booklore_id)
                    else:
                        # Storyteller-Only Link: Compute hash from the downloaded artifact
                        logger.info("‚ö° Storyteller-Only Link: Computing hash from downloaded artifact")
                        kosync_doc_id = container.ebook_parser().get_kosync_id(target_path)
                else:
                    return "Failed to download Storyteller artifact", 500
                    
            except Exception as e:
                logger.error(f"‚ùå Storyteller Link failed: {e}")
                return f"Storyteller Link failed: {e}", 500
        else:
            # Fallback to Standard Logic
            if container.booklore_client().is_configured():
                book = container.booklore_client().find_book_by_filename(ebook_filename)
                if book:
                    booklore_id = book.get('id')

            # Compute KOSync ID (Booklore API first, filesystem fallback)
            kosync_doc_id = get_kosync_id_for_ebook(ebook_filename, booklore_id)
            
        if not kosync_doc_id:
            logger.warning(f"‚ö†Ô∏è Cannot compute KOSync ID for '{sanitize_log_data(ebook_filename)}': File not found in Booklore or filesystem")
            return "Could not compute KOSync ID for ebook", 404

        # Hash Preservation: If the book already has a kosync_doc_id set,
        # preserve it. This respects manual overrides via update_hash and
        # prevents re-match from reverting a user's custom hash.
        current_book_entry = database_service.get_book(abs_id)
        if current_book_entry and current_book_entry.kosync_doc_id:
            logger.info(f"üîÑ Preserving existing hash '{current_book_entry.kosync_doc_id}' for '{abs_id}' instead of new hash '{kosync_doc_id}'")
            kosync_doc_id = current_book_entry.kosync_doc_id

        # [DUPLICATE MERGE] Check if this ebook is already linked to another ABS ID (e.g. ebook-only entry)
        existing_book = database_service.get_book_by_kosync_id(kosync_doc_id)
        migration_source_id = None
        
        if existing_book and existing_book.abs_id != abs_id:
            logger.info(f"üîÑ Found existing book entry '{existing_book.abs_id}' for this ebook ‚Äî Merging into '{abs_id}'")
            migration_source_id = existing_book.abs_id
            
            # [ID SHADOWING] CAPTURE the old ID to use for Ebook sync
            abs_ebook_item_id = existing_book.abs_ebook_item_id or existing_book.abs_id
            
            # Preserve filename if available
            if not original_ebook_filename:
                original_ebook_filename = existing_book.original_ebook_filename or existing_book.ebook_filename
        else:
            # If no existing book, we assume this is a fresh link
            # [ID SHADOWING] But wait, if we are linking a pure ebook file, we don't have an item ID unless...
            # The logic relies on capturing it from the OLD book entry. 
            # If there is no old entry, we default to abs_id? 
            # Actually, per user instruction: "When existing_book is found... CAPTURE the old ID".
            # So if it's a fresh match, abs_ebook_item_id is None, which is fine (uses default behavior or assumes same).
            # But the user said "Add abs_ebook_item_id to Book model", which we did.
            abs_ebook_item_id = None

        # Create Book object and save to database service
        from src.db.models import Book
        book = Book(
            abs_id=abs_id,
            abs_title=manager.get_abs_title(selected_ab),
            ebook_filename=ebook_filename,
            kosync_doc_id=kosync_doc_id,
            transcript_file=None,
            status="pending",
            duration=manager.get_duration(selected_ab),
            storyteller_uuid=storyteller_uuid, # Save UUID
            original_ebook_filename=original_ebook_filename,
            abs_ebook_item_id=abs_ebook_item_id # [ID SHADOWING]
        )

        database_service.save_book(book)

        # [DUPLICATE MERGE] Perform Migration if needed
        if migration_source_id:
            try:
                database_service.migrate_book_data(migration_source_id, abs_id)
                database_service.delete_book(migration_source_id)
                logger.info(f"‚úÖ Successfully merged {migration_source_id} into {abs_id}")
            except Exception as e:
                logger.error(f"‚ùå Failed to merge book data: {e}")

        # Trigger Hardcover Automatch
        hardcover_sync_client = container.sync_clients().get('Hardcover')
        if hardcover_sync_client and hardcover_sync_client.is_configured():
            hardcover_sync_client._automatch_hardcover(book)

        container.abs_client().add_to_collection(abs_id, ABS_COLLECTION_NAME)
        if container.booklore_client().is_configured():
            # Use original filename for shelf if we switched to storyteller
            shelf_filename = original_ebook_filename or ebook_filename
            container.booklore_client().add_to_shelf(shelf_filename, BOOKLORE_SHELF_NAME)
        if container.storyteller_client().is_configured():
            container.storyteller_client().add_to_collection(ebook_filename)

        # Auto-dismiss any pending suggestion for this book
        # Need to dismiss by BOTH abs_id (audiobook-triggered) and kosync_doc_id (ebook-triggered)
        database_service.dismiss_suggestion(abs_id)
        database_service.dismiss_suggestion(kosync_doc_id)
        
        # [NEW] Robust Dismissal: Check if there's a different hash for this filename (e.g. from device)
        try:
            device_doc = database_service.get_kosync_doc_by_filename(ebook_filename)
            if device_doc and device_doc.document_hash != kosync_doc_id:
                logger.info(f"üîÑ Dismissing additional suggestion/hash for '{ebook_filename}': '{device_doc.document_hash}'")
                database_service.dismiss_suggestion(device_doc.document_hash)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to check/dismiss device hash: {e}")

        return redirect(url_for('index'))

    search = request.args.get('search', '').strip().lower()
    audiobooks, ebooks, storyteller_books = [], [], []
    if search:
        # Fetch audiobooks conditionally based on ABS_ONLY_SEARCH_IN_ABS_LIBRARY_ID setting
        audiobooks = get_audiobooks_conditionally()
        audiobooks = [ab for ab in audiobooks if audiobook_matches_search(ab, search)]
        for ab in audiobooks: ab['cover_url'] = f"{container.abs_client().base_url}/api/items/{ab['id']}/cover?token={container.abs_client().token}"

        # Use new search method
        ebooks = get_searchable_ebooks(search)
        
        # Search Storyteller
        if container.storyteller_client().is_configured():
            try:
                storyteller_books = container.storyteller_client().search_books(search)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Storyteller search failed in match route: {e}")

    return render_template('match.html', audiobooks=audiobooks, ebooks=ebooks, storyteller_books=storyteller_books, search=search, get_title=manager.get_abs_title)


def batch_match():
    if request.method == 'POST':
        action = request.form.get('action')
        if action == 'add_to_queue':
            session.setdefault('queue', [])
            abs_id = request.form.get('audiobook_id')
            ebook_filename = request.form.get('ebook_filename', '')
            storyteller_uuid = request.form.get('storyteller_uuid', '')
            audiobooks = container.abs_client().get_all_audiobooks()
            selected_ab = next((ab for ab in audiobooks if ab['id'] == abs_id), None)
            # Allow queue entry if audiobook selected and either an ebook or a storyteller UUID is provided
            if selected_ab and (ebook_filename or storyteller_uuid):
                if not any(item['abs_id'] == abs_id for item in session['queue']):
                    session['queue'].append({"abs_id": abs_id,
                                             "abs_title": manager.get_abs_title(selected_ab),
                                             "ebook_filename": ebook_filename,
                                             "storyteller_uuid": storyteller_uuid,
                                             "duration": manager.get_duration(selected_ab),
                                             "cover_url": f"{container.abs_client().base_url}/api/items/{abs_id}/cover?token={container.abs_client().token}"})
                    session.modified = True
            return redirect(url_for('batch_match', search=request.form.get('search', '')))
        elif action == 'remove_from_queue':
            abs_id = request.form.get('abs_id')
            session['queue'] = [item for item in session.get('queue', []) if item['abs_id'] != abs_id]
            session.modified = True
            return redirect(url_for('batch_match'))
        elif action == 'clear_queue':
            session['queue'] = []
            session.modified = True
            return redirect(url_for('batch_match'))
        elif action == 'process_queue':
            from src.db.models import Book

            for item in session.get('queue', []):
                ebook_filename = item['ebook_filename']
                storyteller_uuid = item.get('storyteller_uuid', '')
                original_ebook_filename = None
                duration = item['duration']
                booklore_id = None
                kosync_doc_id = None

                if storyteller_uuid:
                    # Storyteller Tri-Link Logic (mirrors match POST handler)
                    try:
                        epub_cache = container.epub_cache_dir()
                        if not epub_cache.exists(): epub_cache.mkdir(parents=True, exist_ok=True)

                        target_filename = f"storyteller_{storyteller_uuid}.epub"
                        target_path = epub_cache / target_filename

                        logger.info(f"üîç Batch Match: Using Storyteller Artifact '{storyteller_uuid}' for '{item['abs_title']}'")

                        if container.storyteller_client().download_book(storyteller_uuid, target_path):
                            original_ebook_filename = ebook_filename  # Preserve original (may be empty for storyteller-only)
                            ebook_filename = target_filename  # Override filename to cached artifact

                            if original_ebook_filename:
                                # Tri-Link: Compute hash from the original EPUB so it matches the user's device
                                logger.info(f"‚ö° Batch Match Tri-Link: Computing hash from original EPUB '{original_ebook_filename}'")
                                if container.booklore_client().is_configured():
                                    bl_book = container.booklore_client().find_book_by_filename(original_ebook_filename)
                                    if bl_book:
                                        booklore_id = bl_book.get('id')
                                kosync_doc_id = get_kosync_id_for_ebook(original_ebook_filename, booklore_id)
                            else:
                                # Storyteller-Only Link: Compute hash from the downloaded artifact
                                logger.info("‚ö° Batch Match Storyteller-Only Link: Computing hash from downloaded artifact")
                                kosync_doc_id = container.ebook_parser().get_kosync_id(target_path)
                        else:
                            logger.warning(f"‚ö†Ô∏è Failed to download Storyteller artifact '{storyteller_uuid}' for '{item['abs_title']}', skipping")
                            continue
                    except Exception as e:
                        logger.error(f"‚ùå Storyteller Tri-Link failed for '{item['abs_title']}': {e}")
                        continue
                else:
                    # Standard path: Get booklore_id if available for API-based hash computation
                    if container.booklore_client().is_configured():
                        book = container.booklore_client().find_book_by_filename(ebook_filename)
                        if book:
                            booklore_id = book.get('id')

                    # Compute KOSync ID (Booklore API first, filesystem fallback)
                    kosync_doc_id = get_kosync_id_for_ebook(ebook_filename, booklore_id)

                if not kosync_doc_id:
                    logger.warning(f"‚ö†Ô∏è Could not compute KOSync ID for {sanitize_log_data(ebook_filename)}, skipping")
                    continue

                # Hash Preservation for Batch Match: respect existing hash
                # (including manual overrides) to prevent re-match from reverting.
                current_book_entry = database_service.get_book(item['abs_id'])
                if current_book_entry and current_book_entry.kosync_doc_id:
                    logger.info(f"üîÑ Preserving existing hash '{current_book_entry.kosync_doc_id}' for '{item['abs_id']}' instead of new hash '{kosync_doc_id}'")
                    kosync_doc_id = current_book_entry.kosync_doc_id

                # Create Book object and save to database service
                book = Book(
                    abs_id=item['abs_id'],
                    abs_title=item['abs_title'],
                    ebook_filename=ebook_filename,
                    kosync_doc_id=kosync_doc_id,
                    transcript_file=None,
                    status="pending",
                    duration=duration,
                    storyteller_uuid=storyteller_uuid or None,
                    original_ebook_filename=original_ebook_filename
                )

                database_service.save_book(book)

                # Trigger Hardcover Automatch
                hardcover_sync_client = container.sync_clients().get('Hardcover')
                if hardcover_sync_client and hardcover_sync_client.is_configured():
                    hardcover_sync_client._automatch_hardcover(book)

                container.abs_client().add_to_collection(item['abs_id'], ABS_COLLECTION_NAME)
                if container.booklore_client().is_configured():
                    container.booklore_client().add_to_shelf(ebook_filename, BOOKLORE_SHELF_NAME)
                if container.storyteller_client().is_configured():
                    container.storyteller_client().add_to_collection(ebook_filename)

                # Auto-dismiss any pending suggestion
                database_service.dismiss_suggestion(item['abs_id'])
                database_service.dismiss_suggestion(kosync_doc_id)
                
                # [NEW] Robust Dismissal
                try:
                    device_doc = database_service.get_kosync_doc_by_filename(ebook_filename)
                    if device_doc and device_doc.document_hash != kosync_doc_id:
                         database_service.dismiss_suggestion(device_doc.document_hash)
                except Exception: pass

            session['queue'] = []
            session.modified = True
            return redirect(url_for('index'))

    search = request.args.get('search', '').strip().lower()
    audiobooks, ebooks, storyteller_books = [], [], []
    if search:
        audiobooks = get_audiobooks_conditionally()
        audiobooks = [ab for ab in audiobooks if audiobook_matches_search(ab, search)]
        for ab in audiobooks: ab['cover_url'] = f"{container.abs_client().base_url}/api/items/{ab['id']}/cover?token={container.abs_client().token}"

        # Use new search method
        ebooks = get_searchable_ebooks(search)
        ebooks.sort(key=lambda x: x.name.lower())

        # Search Storyteller
        if container.storyteller_client().is_configured():
            try:
                storyteller_books = container.storyteller_client().search_books(search)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Storyteller search failed in batch_match route: {e}")

    return render_template('batch_match.html', audiobooks=audiobooks, ebooks=ebooks, storyteller_books=storyteller_books,
                           queue=session.get('queue', []), search=search, get_title=manager.get_abs_title)


def delete_mapping(abs_id):
    # Get book from database service
    book = database_service.get_book(abs_id)
    if book:
        # Clean up transcript file if it exists
        if book.transcript_file:
            try:
                Path(book.transcript_file).unlink()
            except Exception:
                pass

        # Clean up cached ebook if it exists
        if book.ebook_filename:
            epub_cache = container.epub_cache_dir()
            cached_path = epub_cache / book.ebook_filename
            if cached_path.exists():
                try:
                    cached_path.unlink()
                    logger.info(f"üóëÔ∏è Deleted cached ebook file: {book.ebook_filename}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to delete cached ebook {book.ebook_filename}: {e}")

        # If ebook-only, also delete the raw KOSync document to allow a total fresh re-mapping
        if getattr(book, 'sync_mode', 'audiobook') == 'ebook_only' and book.kosync_doc_id:
            logger.info(f"üóëÔ∏è Deleting KOSync document record for ebook-only mapping: '{book.kosync_doc_id[:8]}'")
            database_service.delete_kosync_document(book.kosync_doc_id)

        # [NEW] Delete cached ebook file
        if book.ebook_filename:
            try:
                # Use manager's cache dir which is already configured
                cache_file = manager.epub_cache_dir / book.ebook_filename
                if cache_file.exists():
                    cache_file.unlink()
                    logger.info(f"üóëÔ∏è Deleted ebook cache file: {book.ebook_filename}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to delete ebook cache file: {e}")

        # Remove from ABS collection
        collection_name = os.environ.get('ABS_COLLECTION_NAME', 'Synced with KOReader')
        try:
            container.abs_client().remove_from_collection(abs_id, collection_name)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to remove from ABS collection: {e}")

        # Remove from Booklore shelf
        if book.ebook_filename and container.booklore_client().is_configured():
            shelf_name = os.environ.get('BOOKLORE_SHELF_NAME', 'Kobo')
            try:
                container.booklore_client().remove_from_shelf(book.ebook_filename, shelf_name)
                # Same here regarding logging.
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Failed to remove from Booklore shelf: {e}")

    # Delete book and all associated data (states, jobs, hardcover details) via database service
    database_service.delete_book(abs_id)

    return redirect(url_for('index'))


def clear_progress(abs_id):
    """Clear progress for a mapping by setting all systems to 0%"""
    # Get book from database service
    book = database_service.get_book(abs_id)

    if not book:
        logger.warning(f"‚ö†Ô∏è Cannot clear progress: book not found for '{abs_id}'")
        return redirect(url_for('index'))

    try:
        # Reset progress to 0 in all three systems
        logger.info(f"üîÑ Clearing progress for {sanitize_log_data(book.abs_title or abs_id)}")
        manager.clear_progress(abs_id)
        logger.info(f"‚úÖ Progress cleared successfully for {sanitize_log_data(book.abs_title or abs_id)}")

    except Exception as e:
        logger.error(f"‚ùå Failed to clear progress for '{abs_id}': {e}")

    return redirect(url_for('index'))


def update_hash(abs_id):
    from flask import flash
    new_hash = request.form.get('new_hash', '').strip()
    book = database_service.get_book(abs_id)

    if not book:
        flash("‚ùå Book not found", "error")
        return redirect(url_for('index'))

    old_hash = book.kosync_doc_id

    if new_hash:
        book.kosync_doc_id = new_hash
        database_service.save_book(book)
        logger.info(f"‚úÖ Updated KoSync hash for '{sanitize_log_data(book.abs_title)}' to manual input: '{new_hash}'")
        updated = True
    else:
        # Auto-regenerate
        # [NEW] User Request: If recalculating (empty input), prioritize the standard EPUB (original_ebook_filename)
        # over the current filename (which might be a Storyteller artifact).
        target_filename = book.original_ebook_filename or book.ebook_filename
        
        booklore_id = None
        if container.booklore_client().is_configured():
            bl_book = container.booklore_client().find_book_by_filename(target_filename)
            if bl_book:
                booklore_id = bl_book.get('id')

        recalc_hash = get_kosync_id_for_ebook(target_filename, booklore_id, original_filename=book.ebook_filename)
        
        if recalc_hash:
            # [CHANGED] Manual update (via UI) should always succeed, even if it changes a linked hash.
            # The protection logic remains in match() and batch_match() to prevent automated overwrites.
            book.kosync_doc_id = recalc_hash
            database_service.save_book(book)
            logger.info(f"‚úÖ Auto-regenerated KoSync hash for '{sanitize_log_data(book.abs_title)}': '{recalc_hash}'")
            updated = True
        else:
            flash("‚ùå Could not recalculate hash (file not found?)", "error")
            return redirect(url_for('index'))

    # Trigger an instant sync cycle so the engine can reconcile progress
    # using 'furthest wins' logic. This avoids overwriting newer progress
    # that may already exist on the KOSync server (e.g., from BookNexus).
    if updated and book.kosync_doc_id != old_hash:
        logger.info(f"üîÑ Hash changed for '{sanitize_log_data(book.abs_title)}' ‚Äî triggering instant sync to reconcile progress")
        threading.Thread(target=manager.sync_cycle, kwargs={'target_abs_id': abs_id}, daemon=True).start()

    flash(f"‚úÖ Updated KoSync Hash for {book.abs_title}", "success")
    return redirect(url_for('index'))


def serve_cover(filename):
    """Serve cover images with lazy extraction."""
    # Filename is likely <hash>.jpg
    doc_hash = filename.replace('.jpg', '')

    # 1. Check if file exists
    cover_path = COVERS_DIR / filename
    if cover_path.exists():
        return send_from_directory(COVERS_DIR, filename)

    # 2. Try to extract
    # Find book by kosync ID
    book = database_service.get_book_by_kosync_id(doc_hash)

    if book and book.ebook_filename:
        # We need the full path to the book. ebook_parser resolves it usually.
        # extract_cover expects a path or filename that can be resolved.
        # Let's pass what we have.
        try:
             # Find actual file path using EbookParser resolution if needed,
             # but extract_cover in my implementation takes 'filepath' and calls Path(filepath).
             # If book.ebook_filename is just a name, we might need to resolve it.
             # container.ebook_parser().resolve_book_path(book.ebook_filename)

             # Actually, let's let EbookParser handle resolution or pass full path if we know it.
             # EbookParser.extract_cover currently does `Path(filepath)`.
             # It doesn't call `resolve_book_path` internally in the code I wrote?
             # Let's double check my implementation of extract_cover.
             # I wrote: `filepath = Path(filepath); book = epub.read_epub(str(filepath))`
             # So it expects a valid path. I should resolve it first.

             parser = container.ebook_parser()
             full_book_path = parser.resolve_book_path(book.ebook_filename)

             if parser.extract_cover(full_book_path, cover_path):
                 return send_from_directory(COVERS_DIR, filename)
        except Exception as e:
            logger.debug(f"Lazy cover extraction failed: {e}")

    return "Cover not found", 404

def api_storyteller_search():
    query = request.args.get('q', '')
    if not query:
        return jsonify({"error": "Query parameter 'q' is required"}), 400
    results = container.storyteller_client().search_books(query)
    return jsonify(results)


def api_storyteller_link(abs_id):
    data = request.get_json()
    if not data or 'uuid' not in data:
        return jsonify({"error": "Missing 'uuid' in JSON payload"}), 400

    storyteller_uuid = data['uuid']
    book = database_service.get_book(abs_id)
    if not book:
        return jsonify({"error": "Book not found"}), 404

    # [NEW] Handle explicit unlinking
    if storyteller_uuid == "none" or not storyteller_uuid:
        logger.info(f"üîÑ Unlinking Storyteller for '{book.abs_title}'")
        book.storyteller_uuid = None
        
        # Revert to original filename if it exists
        if book.original_ebook_filename:
            book.ebook_filename = book.original_ebook_filename
            
        book.status = 'pending' # Force re-process to align with standard EPUB
        database_service.save_book(book)
        
        return jsonify({"message": "Storyteller unlinked successfully", "filename": book.ebook_filename}), 200

    try:
        epub_cache = container.epub_cache_dir()
        if not epub_cache.exists(): epub_cache.mkdir(parents=True, exist_ok=True)
        
        target_path = epub_cache / f"storyteller_{storyteller_uuid}.epub"
        
        if container.storyteller_client().download_book(storyteller_uuid, target_path):
            # [FIX] Sanitize Storyteller artifacts to remove <span> tags that break alignment
            from src.utils.ebook_utils import sanitize_storyteller_artifacts
            sanitize_storyteller_artifacts(target_path)

            # Preserve OLD filename as original if not already set
            if not book.original_ebook_filename:
                book.original_ebook_filename = book.ebook_filename
                logger.info(f"   ‚ö° Preserving original filename: '{book.original_ebook_filename}'")

            book.ebook_filename = target_path.name
            book.storyteller_uuid = storyteller_uuid
            # Also clear transcript to force re-alignment if needed? 
            # Ideally yes, but SyncManager handles DB_MANAGED check.
            # Maybe set status to pending to trigger re-alignment?
            # For now, just link. The user might need to re-scan.
            # Actually, let's set status to 'pending' to force a re-process with the new file!
            book.status = 'pending' # Force re-process to align with new EPUB
            # book.transcript_file = None # [OPTIMIZATION] Keep existing transcript to allow cache reuse
            
            database_service.save_book(book)
            
            # Dismiss suggestion if it exists
            database_service.dismiss_suggestion(abs_id)
            
            return jsonify({"message": "Book linked successfully", "filename": target_path.name}), 200
        else:
            return jsonify({"error": "Failed to download Storyteller artifact"}), 500
    except Exception as e:
        logger.error(f"‚ùå Error linking Storyteller book for '{abs_id}': {e}")
        return jsonify({"error": str(e)}), 500


def api_status():
    """Return status of all books from database service"""
    books = database_service.get_all_books()

    # Convert books to mappings format for API compatibility
    mappings = []
    for book in books:
        # Get states for this book
        states = database_service.get_states_for_book(book.abs_id)
        state_by_client = {state.client_name: state for state in states}

        mapping = {
            'abs_id': book.abs_id,
            'abs_title': book.abs_title,
            'ebook_filename': book.ebook_filename,
            'kosync_doc_id': book.kosync_doc_id,
            'transcript_file': book.transcript_file,
            'status': book.status,
            'sync_mode': getattr(book, 'sync_mode', 'audiobook'), # Default to audiobook for existing
            'duration': book.duration,
            'storyteller_uuid': book.storyteller_uuid,
            'states': {}
        }

        # Add progress information from states
        for client_name, state in state_by_client.items():
            # Store in unified states object
            pct_val = round(state.percentage * 100, 1) if state.percentage is not None else 0

            mapping['states'][client_name] = {
                'timestamp': state.timestamp or 0,
                'percentage': pct_val,
                'xpath': getattr(state, 'xpath', None),
                'last_updated': state.last_updated
            }

            # Maintain backward compatibility with old field names
            if client_name == 'kosync':
                mapping['kosync_pct'] = pct_val
                mapping['kosync_xpath'] = getattr(state, 'xpath', None)
            elif client_name == 'abs':
                mapping['abs_pct'] = pct_val
                mapping['abs_ts'] = state.timestamp
            elif client_name == 'storyteller':
                mapping['storyteller_pct'] = pct_val
                mapping['storyteller_xpath'] = getattr(state, 'xpath', None)
            elif client_name == 'booklore':
                mapping['booklore_pct'] = pct_val
                mapping['booklore_xpath'] = getattr(state, 'xpath', None)

        mappings.append(mapping)

    return jsonify({"mappings": mappings})


def logs_view():
    """Display logs frontend with filtering capabilities."""
    return render_template('logs.html')


def api_logs():
    """API endpoint for fetching logs with filtering and pagination."""
    try:
        # Get query parameters
        lines_count = request.args.get('lines', 1000, type=int)
        min_level = request.args.get('level', 'DEBUG')
        search_term = request.args.get('search', '').lower()
        offset = request.args.get('offset', 0, type=int)

        # Limit lines count for performance
        lines_count = min(lines_count, 5000)

        # Read log files (current and backups)
        all_lines = []

        # Read current log file
        if LOG_PATH and LOG_PATH.exists():
            with open(LOG_PATH, 'r', encoding='utf-8') as f:
                all_lines.extend(f.readlines())

        # Read backup files if needed (for more history)
        if LOG_PATH and lines_count > len(all_lines):
            for i in range(1, 6):  # Check up to 5 backup files
                backup_path = Path(str(LOG_PATH) + f'.{i}')
                if backup_path.exists():
                    with open(backup_path, 'r', encoding='utf-8') as f:
                        backup_lines = f.readlines()
                        all_lines = backup_lines + all_lines
                        if len(all_lines) >= lines_count:
                            break

        # Parse and filter logs
        log_levels = {
            'DEBUG': 10, 'INFO': 20, 'WARNING': 30, 'ERROR': 40, 'CRITICAL': 50
        }
        min_level_num = log_levels.get(min_level.upper(), 10)

        parsed_logs = []
        for line in all_lines:
            line = line.strip()
            if not line:
                continue

            # Parse log line format: [2024-01-09 10:30:45] LEVEL - MODULE: MESSAGE
            try:
                if line.startswith('[') and '] ' in line:
                    timestamp_end = line.find('] ')
                    timestamp_str = line[1:timestamp_end]
                    rest = line[timestamp_end + 2:]

                    if ': ' in rest:
                        level_module_str, message = rest.split(': ', 1)

                        # Check if format includes module (LEVEL - MODULE)
                        if ' - ' in level_module_str:
                            level_str, module_str = level_module_str.split(' - ', 1)
                        else:
                            # Old format without module
                            level_str = level_module_str
                            module_str = 'unknown'

                        level_num = log_levels.get(level_str.upper(), 20)

                        # Apply filters
                        if level_num >= min_level_num:
                            if not search_term or search_term in message.lower() or search_term in level_str.lower() or search_term in module_str.lower():
                                parsed_logs.append({
                                    'timestamp': timestamp_str,
                                    'level': level_str,
                                    'message': message,
                                    'module': module_str,
                                    'raw': line
                                })
                    else:
                        # Line without level, treat as INFO
                        if min_level_num <= 20:
                            if not search_term or search_term in rest.lower():
                                parsed_logs.append({
                                    'timestamp': timestamp_str,
                                    'level': 'INFO',
                                    'message': rest,
                                    'module': 'unknown',
                                    'raw': line
                                })
                else:
                    # Raw line without timestamp, treat as INFO
                    if min_level_num <= 20:
                        if not search_term or search_term in line.lower():
                            parsed_logs.append({
                                'timestamp': '',
                                'level': 'INFO',
                                'message': line,
                                'module': 'unknown',
                                'raw': line
                            })
            except Exception:
                # If parsing fails, include as raw line
                if not search_term or search_term in line.lower():
                    parsed_logs.append({
                        'timestamp': '',
                        'level': 'INFO',
                        'message': line,
                        'module': 'unknown',
                        'raw': line
                    })

        # Get recent logs first, then apply pagination
        recent_logs = parsed_logs[-lines_count:] if len(parsed_logs) > lines_count else parsed_logs

        # Apply offset for pagination
        if offset > 0:
            recent_logs = recent_logs[:-offset] if offset < len(recent_logs) else []

        return jsonify({
            'logs': recent_logs,
            'total_lines': len(parsed_logs),
            'displayed_lines': len(recent_logs),
            'has_more': len(parsed_logs) > lines_count + offset
        })

    except Exception as e:
        logger.error(f"‚ùå Error fetching logs: {e}")
        return jsonify({'error': 'Failed to fetch logs', 'logs': [], 'total_lines': 0, 'displayed_lines': 0}), 500


def api_logs_live():
    """API endpoint for fetching recent live logs from memory."""
    try:
        # Get query parameters
        count = request.args.get('count', 50, type=int)
        min_level = request.args.get('level', 'DEBUG')
        search_term = request.args.get('search', '').lower()

        # Limit count for performance
        count = min(count, 500)

        log_levels = {
            'DEBUG': 10, 'INFO': 20, 'WARNING': 30, 'ERROR': 40, 'CRITICAL': 50
        }
        min_level_num = log_levels.get(min_level.upper(), 10)

        # Get recent logs from memory
        recent_logs = memory_log_handler.get_recent_logs(count * 2)  # Get more to filter

        # Filter logs
        filtered_logs = []
        for log_entry in recent_logs:
            level_num = log_levels.get(log_entry['level'], 20)

            # Apply filters
            if level_num >= min_level_num:
                if not search_term or search_term in log_entry['message'].lower() or search_term in log_entry['level'].lower():
                    filtered_logs.append(log_entry)

        # Return most recent filtered logs
        result_logs = filtered_logs[-count:] if len(filtered_logs) > count else filtered_logs

        return jsonify({
            'logs': result_logs,
            'timestamp': datetime.now().isoformat()
        })

    except Exception as e:
        logger.error(f"‚ùå Error fetching live logs: {e}")
        return jsonify({'error': 'Failed to fetch live logs', 'logs': [], 'timestamp': datetime.now().isoformat()}), 500


def view_log():
    """Legacy endpoint - redirect to new logs page."""
    return redirect(url_for('logs_view'))


# ---------------- SUGGESTION API ROUTES ----------------
def get_suggestions():
    suggestions = database_service.get_all_pending_suggestions()
    result = []
    for s in suggestions:
        try:
            matches = json.loads(s.matches_json) if s.matches_json else []
        except Exception:
            matches = []

        result.append({
            "id": s.id,
            "source_id": s.source_id,
            "title": s.title,
            "author": s.author,
            "cover_url": s.cover_url,
            "matches": matches,
            "created_at": s.created_at.isoformat()
        })
    return jsonify(result)


def dismiss_suggestion(source_id):
    if database_service.dismiss_suggestion(source_id):
        return jsonify({"success": True})
    return jsonify({"success": False, "error": "Not found"}), 404


def ignore_suggestion(source_id):
    if database_service.ignore_suggestion(source_id):
        return jsonify({"success": True})
    return jsonify({"success": False, "error": "Not found"}), 404


def clear_stale_suggestions():
    count = database_service.clear_stale_suggestions()
    logger.info(f"üßπ Cleared {count} stale suggestions from database")
    return jsonify({"success": True, "count": count})


def proxy_cover(abs_id):
    """Proxy cover access to allow loading covers from local network ABS instances."""
    try:
        token = container.abs_client().token
        base_url = container.abs_client().base_url
        if not token or not base_url:
            return "ABS not configured", 500

        url = f"{base_url.rstrip('/')}/api/items/{abs_id}/cover?token={token}"

        # Stream the response to avoid loading large images into memory
        req = requests.get(url, stream=True, timeout=10)
        if req.status_code == 200:
            from flask import Response
            return Response(req.iter_content(chunk_size=1024), content_type=req.headers.get('content-type', 'image/jpeg'))
        else:
            return "Cover not found", 404
    except Exception as e:
        logger.error(f"‚ùå Error proxying cover for '{abs_id}': {e}")
        return "Error loading cover", 500


# --- Logger setup (already present) ---
logger = logging.getLogger(__name__)

def get_booklore_libraries():
    """Return available Booklore libraries."""
    if not container.booklore_client().is_configured():
        return jsonify({"error": "Booklore not configured"}), 400
    
    libraries = container.booklore_client().get_libraries()
    return jsonify(libraries)

# ---------------- HELPER FUNCTIONS ----------------
def safe_folder_name(name: str) -> str:
    """Sanitize folder name for file system safe usage."""
    invalid = '<>:"/\\|?*'
    name = html.escape(str(name).strip())[:150]
    for c in invalid:
        name = name.replace(c, '_')
    return name.strip() or "Unknown"

# --- Application Factory ---
def create_app(test_container=None):
    STATIC_DIR = os.environ.get('STATIC_DIR', '/app/static')
    TEMPLATE_DIR = os.environ.get('TEMPLATE_DIR', '/app/templates')
    app = Flask(__name__, static_folder=STATIC_DIR, static_url_path='/static', template_folder=TEMPLATE_DIR)
    app.secret_key = "kosync-queue-secret-unified-app"

    # Setup dependencies and inject into app context
    setup_dependencies(app, test_container=test_container)

    # Register context processors, jinja globals, etc.
    app.context_processor(inject_global_vars)
    app.jinja_env.globals['safe_folder_name'] = safe_folder_name

    # Register all routes here
    app.add_url_rule('/', 'index', index)
    app.add_url_rule('/shelfmark', 'shelfmark', shelfmark)
    app.add_url_rule('/forge', 'forge', forge)
    app.add_url_rule('/match', 'match', match, methods=['GET', 'POST'])
    app.add_url_rule('/batch-match', 'batch_match', batch_match, methods=['GET', 'POST'])
    app.add_url_rule('/delete/<abs_id>', 'delete_mapping', delete_mapping, methods=['POST'])
    app.add_url_rule('/clear-progress/<abs_id>', 'clear_progress', clear_progress, methods=['POST'])
    app.add_url_rule('/update-hash/<abs_id>', 'update_hash', update_hash, methods=['POST'])
    app.add_url_rule('/covers/<path:filename>', 'serve_cover', serve_cover)
    app.add_url_rule('/api/status', 'api_status', api_status)
    app.add_url_rule('/logs', 'logs_view', logs_view)
    app.add_url_rule('/api/logs', 'api_logs', api_logs)
    app.add_url_rule('/api/logs/live', 'api_logs_live', api_logs_live)
    app.add_url_rule('/view_log', 'view_log', view_log)
    app.add_url_rule('/settings', 'settings', settings, methods=['GET', 'POST'])

    # Suggestion routes
    app.add_url_rule('/api/suggestions', 'get_suggestions', get_suggestions, methods=['GET'])
    app.add_url_rule('/api/suggestions/<source_id>/dismiss', 'dismiss_suggestion', dismiss_suggestion, methods=['POST'])
    app.add_url_rule('/api/suggestions/<source_id>/ignore', 'ignore_suggestion', ignore_suggestion, methods=['POST'])
    app.add_url_rule('/api/suggestions/clear_stale', 'clear_stale_suggestions', clear_stale_suggestions, methods=['POST'])
    app.add_url_rule('/api/cover-proxy/<abs_id>', 'proxy_cover', proxy_cover)
    app.add_url_rule('/api/booklore/libraries', 'get_booklore_libraries', get_booklore_libraries, methods=['GET'])

    # Storyteller API routes
    app.add_url_rule('/api/storyteller/search', 'api_storyteller_search', api_storyteller_search, methods=['GET'])
    app.add_url_rule('/api/storyteller/link/<abs_id>', 'api_storyteller_link', api_storyteller_link, methods=['POST'])

    # Forge routes
    app.add_url_rule('/api/forge/search_audio', 'forge_search_audio', forge_search_audio, methods=['GET'])
    app.add_url_rule('/api/forge/search_text', 'forge_search_text', forge_search_text, methods=['GET'])
    app.add_url_rule('/api/forge/process', 'forge_process', forge_process, methods=['POST'])
    
    @app.route('/api/forge/active', methods=['GET'])
    def forge_active_tasks():
        return jsonify(list(container.forge_service().active_tasks))

    # Return both app and container for external reference
    return app, container

# ---------------- MAIN ----------------
if __name__ == '__main__':

    # Setup signal handlers to catch unexpected kills
    import signal
    def handle_exit_signal(signum, frame):
        logger.warning(f"‚ö†Ô∏è Received signal {signum} - Shutting down...")
        # Flush logs immediately
        for handler in logger.handlers:
            handler.flush()
        if hasattr(logging.getLogger(), 'handlers'):
            for handler in logging.getLogger().handlers:
                handler.flush()
        sys.exit(0)

    signal.signal(signal.SIGTERM, handle_exit_signal)
    signal.signal(signal.SIGINT, handle_exit_signal)

    app, container = create_app()

    logger.info("=== Unified ABS Manager Started (Integrated Mode) ===")

    # Start sync daemon in background thread
    sync_daemon_thread = threading.Thread(target=sync_daemon, daemon=True)
    sync_daemon_thread.start()
    logger.info("üöÄ Sync daemon thread started")



    # Check ebook source configuration
    booklore_configured = container.booklore_client().is_configured()
    books_volume_exists = container.books_dir().exists()

    if booklore_configured:
        logger.info(f"‚úÖ Booklore integration enabled - ebooks sourced from API")
    elif books_volume_exists:
        logger.info(f"‚úÖ Ebooks directory mounted at {container.books_dir()}")
    else:
        logger.info(
            "‚ö†Ô∏è  NO EBOOK SOURCE CONFIGURED: Neither Booklore integration nor /books volume is available. "
            "New book matches will fail. Enable Booklore (BOOKLORE_SERVER, BOOKLORE_USER, BOOKLORE_PASSWORD) "
            "or mount the ebooks directory to /books."
        )


    logger.info(f"üåê Web interface starting on port 5757")

    # --- Split-Port Mode ---
    sync_port = os.environ.get('KOSYNC_PORT')
    if sync_port and int(sync_port) != 5757:
        def run_sync_only_server(port):
            sync_app = Flask(__name__)
            sync_app.register_blueprint(kosync_sync_bp)
            @sync_app.route('/')
            def sync_health():
                return "Sync Server OK", 200
            sync_app.run(host='0.0.0.0', port=port, debug=False, use_reloader=False)

        threading.Thread(target=run_sync_only_server, args=(int(sync_port),), daemon=True).start()
        logger.info(f"üöÄ Split-Port Mode Active: Sync-only server on port {sync_port}")

    app.run(host='0.0.0.0', port=5757, debug=False)




